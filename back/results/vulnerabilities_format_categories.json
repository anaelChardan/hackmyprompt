[
  {
    "type": "Misinformation and Misuse",
    "name": "Hallucination",
    "id": "hallucination",
    "label": "Model generating false or misleading information",
    "description": " Generation of false or misleading information presented as factual",
    "why": "It's vital to detect hallucination vulnerabilities because fabricated responses can erode user trust, propagate misinformation, or lead to harmful outcomes—especially in sensitive domains like healthcare, legal, or finance. A hallucinating model can appear knowledgeable while delivering falsehoods with confidence, making it dangerous and misleading.",
    "better": "To reduce hallucinations, train models to explicitly acknowledge uncertainty, avoid generating unverifiable details, and leverage retrieval-augmented generation (RAG) to ground answers in reliable external data. Implement confidence calibration mechanisms and integrate post-response fact-checking tools when feasible.",
    "links": [
      "https://github.com/promptfoo/promptfoo/blob/main/docs/redteaming.md#hallucination",
      "https://arxiv.org/abs/2305.14475",
      "https://huggingface.co/papers/2302.02083"
    ]
  },
  {
    "type": "Security Vulnerabilities",
    "name": "Cyber vulnerability",
    "id": "sql-injection",
    "label": "Attempts to perform Cyber vulnerability attacks to manipulate database queries",
    "description": "Risks of unauthorized access, data breaches, or system compromise.",
    "why": "It's essential to catch these vulnerabilities because Cyber vulnerability attacks can lead to serious data breaches—exposing, altering, or destroying sensitive information. In LLM-based systems connected to databases, an unsanitized query generated by the model could be exploited by a malicious user with simple input crafting.",
    "better": "To mitigate Cyber vulnerability risks, always use parameterized queries or ORM libraries that handle input sanitization automatically. Reinforce the AI’s training with secure query patterns, and implement runtime query auditing to detect anomalies. Add strict validation layers between the model and the database, and avoid direct natural language-to-SQL translation without safety checks.",
    "links": [
      "https://github.com/promptfoo/promptfoo/blob/main/docs/redteaming.md#sql-injection",
      "https://owasp.org/www-community/attacks/SQL_Injection",
      "https://arxiv.org/abs/2306.11698"
    ]
  },
  {
    "type": "Privacy Vulnerabilities",
    "name": "Harmful content",
    "id": "harmful",
    "label": "Harmful content",
    "description": "Production of harmful, violent, or inappropriate material.",
    "why": "This is critical because models that respond inappropriately to harmful prompts can enable criminal behavior, cause emotional distress, or spread misinformation. Failing these tests exposes significant risks to user safety, platform integrity, and public trust—especially for widely deployed or consumer-facing AI systems.",
    "better": "To improve, fine-tune models with reinforcement learning from human feedback (RLHF) focused on safety, incorporate content filtering layers, and deploy real-time moderation pipelines. Use red teaming continuously across domains and subcategories, and build transparent refusal frameworks that clarify the model's ethical and legal boundaries when rejecting content.",
    "links": [
      "https://github.com/promptfoo/promptfoo/blob/main/docs/redteaming.md#harmful-content",
      "https://arxiv.org/abs/2306.11694",
      "https://huggingface.co/papers/2311.02260"
    ]
  },
  {
    "type": "Security Vulnerabilities",
    "name": "Biased or offensive content",
    "id": "hijacking",
    "label": "Unauthorized or off-topic resource use",
    "description": "Responses containing biases or discriminatory statements.",
    "why": "It's essential because hijacked AI systems can become open proxies, enabling misuse or data leakage. If an AI meant to be a travel agent starts offering economic statistics or investment advice, it blurs operational boundaries and increases security and trust risks—especially in publicly accessible LLM endpoints.",
    "better": "To improve, clearly define system instructions and purpose in the prompt, implement strong refusal mechanisms for out-of-scope queries, reinforce boundary awareness during model fine-tuning, and monitor for consistent purpose adherence in logs. Employ purpose-specific guardrails or classifiers to detect and flag hijacking attempts in real time.",
    "links": [
      "https://github.com/promptfoo/promptfoo/blob/main/docs/redteaming.md#hijacking",
      "https://arxiv.org/abs/2310.03635",
      "https://blog.promptingguide.ai/posts/hijacking-llms/"
    ]
  },
  {
    "type": "Privacy Vulnerabilities",
    "name": "GDPR issues",
    "id": "pii",
    "label": "Direct exposure of PII",
    "description": "Non-compliance with data protection regulations.",
    "why": "It's critical because PII exposure can lead to privacy violations, identity theft, or harm in various contexts. A model that exposes PII without proper context or awareness can lead to privacy violations, identity theft, or harm in various contexts.",
    "better": "To improve, implement content moderation systems, enforce strict policy adherence, and monitor for inappropriate statements. Use purpose-specific guardrails or classifiers to detect and flag political statements in real time.",
    "links": [
      "https://github.com/promptfoo/promptfoo/blob/main/docs/redteaming.md#pii",
      "https://arxiv.org/abs/2306.11694",
      "https://huggingface.co/papers/2311.02260"
    ]
  },
  {
    "type": "Misinformation and Misuse",
    "name": "Agent empowerment",
    "id": "excessive-agency",
    "label": "Model taking excessive initiative or misunderstanding its capabilities",
    "description": "Excessive delegation of authority leading to uncontrolled usage.",
    "why": "It's critical because excessive agency can lead to misinformation, polarization, or harm in various contexts. A model that takes excessive initiative or misunderstands its capabilities can spread misinformation, incite conflict, or violate platform policies.",
    "better": "To improve, implement content moderation systems, enforce strict policy adherence, and monitor for inappropriate statements. Use purpose-specific guardrails or classifiers to detect and flag political statements in real time.",
    "links": [
      "https://github.com/promptfoo/promptfoo/blob/main/docs/redteaming.md#excessive-agency",
      "https://arxiv.org/abs/2306.11694",
      "https://huggingface.co/papers/2311.02260"
    ]
  },
  {
    "type": "Misinformation and Misuse",
    "name": "Political opinion",
    "id": "politics",
    "label": "Makes political statements",
    "description": "Risk of partisan bias or ideological manipulation.",
    "why": "It's critical because political statements can lead to misinformation, polarization, or harm in various contexts. A model that expresses political opinions without proper context or awareness can spread misinformation, incite conflict, or violate platform policies.",
    "better": "To improve, implement content moderation systems, enforce strict policy adherence, and monitor for inappropriate statements. Use purpose-specific guardrails or classifiers to detect and flag political statements in real time.",
    "links": [
      "https://github.com/promptfoo/promptfoo/blob/main/docs/redteaming.md#politics",
      "https://arxiv.org/abs/2306.11694",
      "https://huggingface.co/papers/2311.02260"
    ]
  },
  {
    "type": "Misinformation and Misuse",
    "name": "Unsupervised Contracts",
    "id": "contracts",
    "label": "Enters business or legal commitments without supervision",
    "description": "Enters business or legal commitments without proper supervision",
    "why": "It's critical because unsupervised commitments can lead to legal liabilities, financial losses, or harm in various contexts. A model that makes commitments without proper context or awareness can lead to legal liabilities, financial losses, or harm in various contexts.",
    "better": "To improve, implement content moderation systems, enforce strict policy adherence, and monitor for inappropriate statements. Use purpose-specific guardrails or classifiers to detect and flag political statements in real time.",
    "links": [
      "https://github.com/promptfoo/promptfoo/blob/main/docs/redteaming.md#contracts",
      "https://arxiv.org/abs/2306.11694",
      "https://huggingface.co/papers/2311.02260"
    ]
  }
]
