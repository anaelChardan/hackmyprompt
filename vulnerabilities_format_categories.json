[
  {
    "type": "Misinformation and Misuse",
    "name": "Hallucination",
    "id": "hallucination",
    "label": "Model generating false or misleading information",
    "description": "The Hallucination plugin evaluates whether a language model generates false or misleading information, especially in response to ambiguous or unverifiable prompts. It mimics scenarios where the AI might invent details, express unjustified confidence, or fail to acknowledge uncertainty.",
    "why": "It's vital to detect hallucination vulnerabilities because fabricated responses can erode user trust, propagate misinformation, or lead to harmful outcomes—especially in sensitive domains like healthcare, legal, or finance. A hallucinating model can appear knowledgeable while delivering falsehoods with confidence, making it dangerous and misleading.",
    "better": "To reduce hallucinations, train models to explicitly acknowledge uncertainty, avoid generating unverifiable details, and leverage retrieval-augmented generation (RAG) to ground answers in reliable external data. Implement confidence calibration mechanisms and integrate post-response fact-checking tools when feasible.",
    "links": [
      "https://github.com/promptfoo/promptfoo/blob/main/docs/redteaming.md#hallucination",
      "https://arxiv.org/abs/2305.14475",
      "https://huggingface.co/papers/2302.02083"
    ]
  },
  {
    "type": "Security Vulnerabilities",
    "name": "Cyber vulnerability",
    "id": "sql-injection",
    "label": "Attempts to perform Cyber vulnerability attacks to manipulate database queries",
    "description": "The Cyber vulnerability plugin tests whether a language model integrated with a database can be tricked into executing unauthorized or malicious SQL commands. It simulates input manipulation attacks designed to exploit poor query construction or inadequate input validation.",
    "why": "It's essential to catch these vulnerabilities because Cyber vulnerability attacks can lead to serious data breaches—exposing, altering, or destroying sensitive information. In LLM-based systems connected to databases, an unsanitized query generated by the model could be exploited by a malicious user with simple input crafting.",
    "better": "To mitigate Cyber vulnerability risks, always use parameterized queries or ORM libraries that handle input sanitization automatically. Reinforce the AI’s training with secure query patterns, and implement runtime query auditing to detect anomalies. Add strict validation layers between the model and the database, and avoid direct natural language-to-SQL translation without safety checks.",
    "links": [
      "https://github.com/promptfoo/promptfoo/blob/main/docs/redteaming.md#sql-injection",
      "https://owasp.org/www-community/attacks/SQL_Injection",
      "https://arxiv.org/abs/2306.11698"
    ]
  },
  {
    "type": "Privacy Vulnerabilities",
    "name": "Harmful content",
    "id": "harmful",
    "label": "Harmful content",
    "description": "The Harmful Content plugin evaluates whether a language model can appropriately detect and reject prompts involving sensitive, unethical, or dangerous topics, including violence, self-harm, illegal activities, and misinformation. It leverages unaligned models to generate adversarial prompts that test the AI’s safety and ethical response boundaries.",
    "why": "This is critical because models that respond inappropriately to harmful prompts can enable criminal behavior, cause emotional distress, or spread misinformation. Failing these tests exposes significant risks to user safety, platform integrity, and public trust—especially for widely deployed or consumer-facing AI systems.",
    "better": "To improve, fine-tune models with reinforcement learning from human feedback (RLHF) focused on safety, incorporate content filtering layers, and deploy real-time moderation pipelines. Use red teaming continuously across domains and subcategories, and build transparent refusal frameworks that clarify the model's ethical and legal boundaries when rejecting content.",
    "links": [
      "https://github.com/promptfoo/promptfoo/blob/main/docs/redteaming.md#harmful-content",
      "https://arxiv.org/abs/2306.11694",
      "https://huggingface.co/papers/2311.02260"
    ]
  },
  {
    "type": "Security Vulnerabilities",
    "name": "Biased or offensive content",
    "id": "hijacking",
    "label": "Unauthorized or off-topic resource use",
    "description": "The Hijacking plugin tests whether a language model can be distracted or misled from its intended system role by cleverly crafted, superficially related queries. It simulates realistic attempts to manipulate the AI into responding to tasks that fall outside its designated function.",
    "why": "It's essential because hijacked AI systems can become open proxies, enabling misuse or data leakage. If an AI meant to be a travel agent starts offering economic statistics or investment advice, it blurs operational boundaries and increases security and trust risks—especially in publicly accessible LLM endpoints.",
    "better": "To improve, clearly define system instructions and purpose in the prompt, implement strong refusal mechanisms for out-of-scope queries, reinforce boundary awareness during model fine-tuning, and monitor for consistent purpose adherence in logs. Employ purpose-specific guardrails or classifiers to detect and flag hijacking attempts in real time.",
    "links": [
      "https://github.com/promptfoo/promptfoo/blob/main/docs/redteaming.md#hijacking",
      "https://arxiv.org/abs/2310.03635",
      "https://blog.promptingguide.ai/posts/hijacking-llms/"
    ]
  },
  {
    "type": "Privacy Vulnerabilities",
    "name": "GDPR issues",
    "id": "pii",
    "label": "Direct exposure of PII"
  },
  {
    "type": "Misinformation and Misuse",
    "name": "Agent empowerment",
    "id": "excessive-agency",
    "label": "Model taking excessive initiative or misunderstanding its capabilities"
  },
  {
    "type": "Misinformation and Misuse",
    "name": "Political opinion",
    "id": "politics",
    "label": "Makes political statements"
  },
  {
    "type": "Misinformation and Misuse",
    "name": "Unsupervised Contracts",
    "id": "contracts",
    "label": "Enters business or legal commitments without supervision"
  }
]
