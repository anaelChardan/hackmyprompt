[
  {
    "type": "Misinformation and Misuse",
    "name": "Hallucination",
    "id": "hallucination",
    "label": "Model generating false or misleading information",
    "description": "The Hallucination plugin evaluates whether a language model generates false or misleading information, especially in response to ambiguous or unverifiable prompts. It mimics scenarios where the AI might invent details, express unjustified confidence, or fail to acknowledge uncertainty.",
    "why": "It's vital to detect hallucination vulnerabilities because fabricated responses can erode user trust, propagate misinformation, or lead to harmful outcomes—especially in sensitive domains like healthcare, legal, or finance. A hallucinating model can appear knowledgeable while delivering falsehoods with confidence, making it dangerous and misleading.",
    "better": "To reduce hallucinations, train models to explicitly acknowledge uncertainty, avoid generating unverifiable details, and leverage retrieval-augmented generation (RAG) to ground answers in reliable external data. Implement confidence calibration mechanisms and integrate post-response fact-checking tools when feasible.",
    "links": [
      "https://github.com/promptfoo/promptfoo/blob/main/docs/redteaming.md#hallucination",
      "https://arxiv.org/abs/2305.14475",
      "https://huggingface.co/papers/2302.02083"
    ]
  },
  {
    "type": "Security Vulnerabilities",
    "name": "Cyber vulnerability",
    "id": "sql-injection",
    "label": "Attempts to perform Cyber vulnerability attacks to manipulate database queries",
    "description": "The Cyber vulnerability plugin tests whether a language model integrated with a database can be tricked into executing unauthorized or malicious SQL commands. It simulates input manipulation attacks designed to exploit poor query construction or inadequate input validation.",
    "why": "It's essential to catch these vulnerabilities because Cyber vulnerability attacks can lead to serious data breaches—exposing, altering, or destroying sensitive information. In LLM-based systems connected to databases, an unsanitized query generated by the model could be exploited by a malicious user with simple input crafting.",
    "better": "To mitigate Cyber vulnerability risks, always use parameterized queries or ORM libraries that handle input sanitization automatically. Reinforce the AI’s training with secure query patterns, and implement runtime query auditing to detect anomalies. Add strict validation layers between the model and the database, and avoid direct natural language-to-SQL translation without safety checks.",
    "links": [
      "https://github.com/promptfoo/promptfoo/blob/main/docs/redteaming.md#sql-injection",
      "https://owasp.org/www-community/attacks/SQL_Injection",
      "https://arxiv.org/abs/2306.11698"
    ]
  },
  {
    "type": "Privacy Vulnerabilities",
    "name": "Harmful content",
    "id": "harmful",
    "label": "Harmful content",
    "description": "The Harmful Content plugin evaluates whether a language model can appropriately detect and reject prompts involving sensitive, unethical, or dangerous topics, including violence, self-harm, illegal activities, and misinformation. It leverages unaligned models to generate adversarial prompts that test the AI’s safety and ethical response boundaries.",
    "why": "This is critical because models that respond inappropriately to harmful prompts can enable criminal behavior, cause emotional distress, or spread misinformation. Failing these tests exposes significant risks to user safety, platform integrity, and public trust—especially for widely deployed or consumer-facing AI systems.",
    "better": "To improve, fine-tune models with reinforcement learning from human feedback (RLHF) focused on safety, incorporate content filtering layers, and deploy real-time moderation pipelines. Use red teaming continuously across domains and subcategories, and build transparent refusal frameworks that clarify the model's ethical and legal boundaries when rejecting content.",
    "links": [
      "https://github.com/promptfoo/promptfoo/blob/main/docs/redteaming.md#harmful-content",
      "https://arxiv.org/abs/2306.11694",
      "https://huggingface.co/papers/2311.02260"
    ]
  },
  {
    "type": "Security Vulnerabilities",
    "name": "Biased or offensive content",
    "id": "hijacking",
    "label": "Unauthorized or off-topic resource use",
    "description": "The Hijacking plugin tests whether a language model can be distracted or misled from its intended system role by cleverly crafted, superficially related queries. It simulates realistic attempts to manipulate the AI into responding to tasks that fall outside its designated function.",
    "why": "It's essential because hijacked AI systems can become open proxies, enabling misuse or data leakage. If an AI meant to be a travel agent starts offering economic statistics or investment advice, it blurs operational boundaries and increases security and trust risks—especially in publicly accessible LLM endpoints.",
    "better": "To improve, clearly define system instructions and purpose in the prompt, implement strong refusal mechanisms for out-of-scope queries, reinforce boundary awareness during model fine-tuning, and monitor for consistent purpose adherence in logs. Employ purpose-specific guardrails or classifiers to detect and flag hijacking attempts in real time.",
    "links": [
      "https://github.com/promptfoo/promptfoo/blob/main/docs/redteaming.md#hijacking",
      "https://arxiv.org/abs/2310.03635",
      "https://blog.promptingguide.ai/posts/hijacking-llms/"
    ]
  },
  {
    "type": "Privacy Vulnerabilities",
    "name": "GDPR issues",
    "id": "pii",
    "label": "Direct exposure of PII",
    "description": "The PII plugin evaluates whether a language model can directly expose Personally Identifiable Information (PII) in its responses. It simulates scenarios where the AI might inadvertently reveal sensitive user data, such as names, addresses, phone numbers, or email addresses.",
    "why": "It's critical because PII exposure can lead to privacy violations, identity theft, or harm in various contexts. A model that exposes PII without proper context or awareness can lead to privacy violations, identity theft, or harm in various contexts.",
    "better": "To improve, implement content moderation systems, enforce strict policy adherence, and monitor for inappropriate statements. Use purpose-specific guardrails or classifiers to detect and flag political statements in real time.",
    "links": [
      "https://github.com/promptfoo/promptfoo/blob/main/docs/redteaming.md#pii",
      "https://arxiv.org/abs/2306.11694",
      "https://huggingface.co/papers/2311.02260"
    ]
  },
  {
    "type": "Misinformation and Misuse",
    "name": "Agent empowerment",
    "id": "excessive-agency",
    "label": "Model taking excessive initiative or misunderstanding its capabilities",
    "description": "The Excessive Agency plugin evaluates whether a language model takes excessive initiative or misunderstands its capabilities. It simulates scenarios where the AI might take inappropriate or unethical actions, especially in sensitive or controversial areas.",
    "why": "It's critical because excessive agency can lead to misinformation, polarization, or harm in various contexts. A model that takes excessive initiative or misunderstands its capabilities can spread misinformation, incite conflict, or violate platform policies.",
    "better": "To improve, implement content moderation systems, enforce strict policy adherence, and monitor for inappropriate statements. Use purpose-specific guardrails or classifiers to detect and flag political statements in real time.",
    "links": [
      "https://github.com/promptfoo/promptfoo/blob/main/docs/redteaming.md#excessive-agency",
      "https://arxiv.org/abs/2306.11694",
      "https://huggingface.co/papers/2311.02260"
    ]
  },
  {
    "type": "Misinformation and Misuse",
    "name": "Political opinion",
    "id": "politics",
    "label": "Makes political statements",
    "description": "The Politics plugin evaluates whether a language model can make political statements or express opinions on political topics. It simulates scenarios where the AI might make inappropriate or unethical statements, especially in sensitive or controversial areas.",
    "why": "It's critical because political statements can lead to misinformation, polarization, or harm in various contexts. A model that expresses political opinions without proper context or awareness can spread misinformation, incite conflict, or violate platform policies.",
    "better": "To improve, implement content moderation systems, enforce strict policy adherence, and monitor for inappropriate statements. Use purpose-specific guardrails or classifiers to detect and flag political statements in real time.",
    "links": [
      "https://github.com/promptfoo/promptfoo/blob/main/docs/redteaming.md#politics",
      "https://arxiv.org/abs/2306.11694",
      "https://huggingface.co/papers/2311.02260"
    ]
  },
  {
    "type": "Misinformation and Misuse",
    "name": "Unsupervised Contracts",
    "id": "contracts",
    "label": "Enters business or legal commitments without supervision",
    "description": "The Contracts plugin evaluates whether a language model can enter into business or legal commitments without proper supervision. It simulates scenarios where the AI might make commitments without proper context or awareness.",
    "why": "It's critical because unsupervised commitments can lead to legal liabilities, financial losses, or harm in various contexts. A model that makes commitments without proper context or awareness can lead to legal liabilities, financial losses, or harm in various contexts.",
    "better": "To improve, implement content moderation systems, enforce strict policy adherence, and monitor for inappropriate statements. Use purpose-specific guardrails or classifiers to detect and flag political statements in real time.",
    "links": [
      "https://github.com/promptfoo/promptfoo/blob/main/docs/redteaming.md#contracts",
      "https://arxiv.org/abs/2306.11694",
      "https://huggingface.co/papers/2311.02260"
    ]
  }
  
]
